{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://tips-memo.com/python-ae\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.17\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/resort/rikako/autoencoder'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---read_training_wav---\n",
      "現在100.0%完了\n",
      "---read_valid_wav---\n",
      "現在100.0%完了\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_mel(wav, sr, n_mels=64, hop_length=160, n_fft=512): #(timeframe, mel_dim) \n",
    "    audio, _ = librosa.load(wav, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels , hop_length=160, n_fft=512).T\n",
    "    return mel\n",
    "\n",
    "###\n",
    "def data_list(path, sr, n_mels=64, hop_length=160, n_fft=512): #620\n",
    "    wav_list = glob.glob(path)\n",
    "    size = len(wav_list)\n",
    "    data = np.ones((1, n_mels))\n",
    "    count= 0\n",
    "    for wavname in wav_list:\n",
    "        component = extract_mel(wavname, sr=sr, n_mels=64, hop_length=160, n_fft=512)\n",
    "        data = np.concatenate([data, component], axis=0)\n",
    "        count += 1\n",
    "        sys.stdout.write(\"\\r%s\" % \"現在\"+str(np.around((count/len(wav_list))*100 , 2))+\"%完了\")\n",
    "        sys.stdout.flush()\n",
    "    return data[1:], size\n",
    "####\n",
    "# pathの設定\n",
    "train_wav_path= \"./train/*.wav\"\n",
    "valid_wav_path = \"./valid/*.wav\"\n",
    "                                                          \n",
    "\n",
    "# パラメータの保存先指定\n",
    "out_audio_dir = \"./data/audio/out_audio_dir/\"\n",
    "if not os.path.exists(out_audio_dir):\n",
    "    os.makedirs(out_audio_dir)\n",
    "\n",
    "\n",
    "# wavデータの一括読み込み\n",
    "# 今回はメル周波数スペクトログラムを15500Hz, 620次元取得して全データに関して縦に並べている\n",
    "print(\"---read_training_wav---\")\n",
    "train_data_list, size_train = data_list(train_wav_path, sr=16000, hop_length=160, n_mels=64, n_fft=512)\n",
    "print(\"\\n---read_valid_wav---\")\n",
    "valid_data_list, size_test = data_list(valid_wav_path, sr=16000, hop_length=160, n_mels=64, n_fft=512)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data_list.pickle', mode='wb') as f:\n",
    "        pickle.dump(train_data_list, f)\n",
    "\n",
    "    with open('valid_data_list.pickle', mode='wb') as f:\n",
    "        pickle.dump(valid_data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() : True\n"
     ]
    }
   ],
   "source": [
    "print('torch.cuda.is_available() :',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 13 17:43:14 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 33%   41C    P8    11W / 180W |    129MiB /  8110MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      2162      G   /usr/lib/xorg/Xorg                            68MiB |\r\n",
      "|    0      2623      G   /usr/bin/gnome-shell                          49MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_params_dir = \"./data/models/params/\"\n",
    "out_figs_dir = \"./data/models/images/\"\n",
    "out_texts_dir = \"./data/models/texts/\"\n",
    "in_audio_dir = \"./data/audio/\"\n",
    "\n",
    "if not os.path.exists(out_params_dir):\n",
    "    os.makedirs(out_params_dir)\n",
    "if not os.path.exists(out_figs_dir):\n",
    "    os.makedirs(out_figs_dir)\n",
    "if not os.path.exists(out_texts_dir):\n",
    "    os.makedirs(out_texts_dir)\n",
    "if not os.path.exists(in_audio_dir):\n",
    "    os.makedirs(in_audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://tasotasoso.hatenablog.com/entry/2020/01/12/184130?utm_source=feed #これも参考？\n",
    "#https://qiita.com/mathlive/items/2a512831878b8018db02 #データセットの作り方\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ExpandDatasetは1つの時間フレームに対して前後10フレームを取得するようなデータセット\n",
    "class ExpandDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.transform = transform\n",
    "        print(self.transform)\n",
    "        self.data = data\n",
    "        print(self.data)\n",
    "        self.data_num = len(data)\n",
    "        if self.data.ndim==2:\n",
    "            self.pad_data_fr = data[:10][::-1] #0~9行目までを\n",
    "            self.pad_data_bc = data[-10:][::-1]\n",
    "            self.pad_data = np.concatenate([self.pad_data_fr, data, self.pad_data_bc], axis=0)\n",
    "        elif self.data.ndim==3:\n",
    "            self.pad_data_fr = data[:,:10,:][:,::-1,:]\n",
    "            self.pad_data_bc = data[:,-10:,:][:,::-1,:]\n",
    "            self.pad_data = np.concatenate([self.pad_data_fr, data, self.pad_data_bc], axis=1)    \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            if self.data.ndim==2:\n",
    "                #__getitem__が呼ばれると,idxから20取ってきて，一次元配列に加工するƒ\n",
    "                out_data = self.transform(self.pad_data)[0][idx:idx+20].flatten()\n",
    "            elif self.data.ndim==3:\n",
    "                index = int(random.uniform(0,self.data.shape[1]))\n",
    "                out_data = self.transform(self.pad_data)[:,idx, index:index+20].flatten()\n",
    "        else:\n",
    "            print(\"transformを使用しテンソル化してください\")\n",
    "        return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dense_enc1 = nn.Linear(1280, 1200)\n",
    "        self.bn1 = nn.BatchNorm1d(1200)\n",
    "        self.dense_enc2 = nn.Linear(1200, 1100)\n",
    "        self.bn2 = nn.BatchNorm1d(1100)\n",
    "        self.dense_enc3 = nn.Linear(1100,1024)\n",
    "    \n",
    "        self.dense_dec1 = nn.Linear(1024,1100)\n",
    "        self.bn4 = nn.BatchNorm1d(1100)\n",
    "        self.dense_dec2 = nn.Linear(1100, 1200)\n",
    "        self.bn5 = nn.BatchNorm1d(1200)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense_dec3 = nn.Linear(1200, 1280)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.dense_enc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.dense_enc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dense_enc3(x)\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.dense_dec1(x))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.dense_dec2(x))\n",
    "        x = self.bn5(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dense_dec3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "[[4.50765656e-04 5.30805322e-04 4.66224774e-05 ... 2.60053270e-07\n",
      "  2.71601010e-07 1.05517472e-07]\n",
      " [4.18963784e-04 4.02063539e-04 1.58382143e-04 ... 5.17296712e-07\n",
      "  3.66128631e-07 8.42486116e-08]\n",
      " [1.46318285e-03 4.32728790e-04 2.18650584e-05 ... 3.93063090e-07\n",
      "  5.57712269e-07 4.71317421e-08]\n",
      " ...\n",
      " [4.11660585e-04 8.21963331e-05 3.10359537e-05 ... 3.40592948e-07\n",
      "  2.95967340e-07 4.87355045e-08]\n",
      " [5.05660602e-04 1.04563136e-04 2.13599142e-05 ... 3.72386722e-07\n",
      "  2.76446372e-07 3.13855359e-08]\n",
      " [5.35616782e-05 3.46706693e-05 3.10118185e-06 ... 3.72152414e-07\n",
      "  8.44074250e-08 1.45347610e-08]]\n",
      "(14959913, 64)\n"
     ]
    }
   ],
   "source": [
    "#データセット作成に必要\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = ExpandDataset(train_data_list, transform)\n",
    "#print(type(train_dataset))\n",
    "print(train_data_list.shape) #(1017741, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3688445, 64)\n",
      "Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "[[1.12353568e-03 2.32042425e-04 8.63318273e-05 ... 2.80149095e-07\n",
      "  1.42585620e-07 2.76045746e-08]\n",
      " [8.54921702e-04 5.76749793e-04 2.92755321e-05 ... 2.58800924e-07\n",
      "  1.49010120e-07 3.15013402e-08]\n",
      " [5.29345532e-04 1.78880757e-04 5.72159406e-05 ... 1.71984652e-07\n",
      "  1.28817035e-07 5.22359400e-08]\n",
      " ...\n",
      " [6.01731881e-04 2.33925020e-04 2.85883125e-05 ... 1.25567317e-06\n",
      "  7.95226242e-07 5.98488228e-08]\n",
      " [1.34566362e-04 2.21855677e-04 5.31124897e-05 ... 7.31991975e-07\n",
      "  4.55107511e-07 9.62500906e-08]\n",
      " [8.93667529e-05 6.47487977e-05 4.50981588e-06 ... 6.42957446e-07\n",
      "  4.22305192e-07 1.39114945e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(valid_data_list.shape) \n",
    "valid_dataset = ExpandDataset(valid_data_list, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 100 #25\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch_size = 100 #25\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_100(epoch):\n",
    "    if epoch <= 100:\n",
    "        return 1\n",
    "    elif 100 < epoch:\n",
    "        return -0.99*(1e-2)*(epoch) + 1.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LambdaLR(optimizer, lr_lambda=func_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Bjarten/early-stopping-pytorch\n",
    "def train(model,train_loader,criterion,optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "        model.zero_grad()\n",
    "        y, z = model(x.float())\n",
    "        \n",
    "        loss = criterion(y.float(), x.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    return train_losses\n",
    "\n",
    "def valid(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    for x in valid_loader:\n",
    "        x = x.to(device)\n",
    "        y, z = model(x.float())\n",
    "        \n",
    "        loss = criterion(y.float(), x.float())\n",
    "        valid_losses.append(loss.item())\n",
    "    return valid_losses\n",
    "\n",
    "#def train_model(model, batch_size, train_loader, valid_loader, patience, criterion, optimizer, num_epochs, PATH):\n",
    "def train_model(model, batch_size, train_loader, valid_loader, patience, criterion, optimizer, num_epochs):\n",
    "        \n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
    "    \n",
    "    for epoch in range(num_epochs):     \n",
    "        train_losses = train(model,train_loader,criterion,optimizer)\n",
    "        valid_losses = valid(model,valid_loader,criterion)\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        \n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(num_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}]' + f'train_loss: {train_loss:.5f}' + f'valid_loss: {valid_loss:.5f}')\n",
    "        print(print_msg)\n",
    "        \n",
    "        \n",
    "        early_stopping(valid_loss,model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "        \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1]train_loss: 0.03990valid_loss: 2.20443\n",
      "Validation loss decreased (inf --> 2.204427).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 #256\n",
    "n_epochs = 1\n",
    "#PATH=\"/opt/resort/rikako/autoencoder/data/models/params\"\n",
    "\n",
    "#train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
    "#train_loader, valid_loader = create_datasets(batch_size)\n",
    "\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "\n",
    "#PATH\n",
    "model, train_loss, valid_loss = train_model(model, batch_size, train_loader, valid_loader, patience, criterion, optimizer, num_epochs)\n",
    "#なんでやろ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = valid_loss.index(min(valid_loss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
